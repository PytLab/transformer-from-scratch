{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer model architecture\n",
    "![](./images/transformer_architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedder(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "        self.embed = torch.nn.Embedding(vocab_size, d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # [123, 0, 23, 5] -> [[..512..], [...512...], ...]\n",
    "        return self.embed(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ PE_{(pos, 2i)} = sin(pos/10000^{2i/d_{model}}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ PE_{(pos, 2i + 1)} = cos(pos/10000^{2i/d_{model}}) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class PositionalEncoder(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, max_seq_len=80):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # create constant positional encoding matrix\n",
    "        pe_matrix = torch.zeros(max_seq_len, d_model)\n",
    "        \n",
    "        for pos in range(max_seq_len):\n",
    "            for i in range(0, d_model, 2):\n",
    "                pe_matrix[pos, i] = math.sin(pos/10000**(2*i/d_model))\n",
    "                pe_matrix[pos, i+1] = math.cos(pos/10000**(2*i/d_model))\n",
    "        pe_matrix = pe_matrix.unsqueeze(0)     # Add one dimension for batch size\n",
    "        self.register_buffer('pe', pe_matrix)  # Register as persistent buffer\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x is a sentence after embedding with dim (batch, number of words, vector dimension)\n",
    "        seq_len = x.size()[1]\n",
    "        x = x + self.pe[:, :seq_len]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaled Dot-Product Attention layer\n",
    "\n",
    "![](images/scaled_dot_product_attention.png)\n",
    "\n",
    "12*512\n",
    "\n",
    "[512..] x M = k_0\n",
    "[...512] x M = k_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Given Query, Key, Value, calculate the final weighted value\n",
    "def scaled_dot_product_attention(q, k, v, mask=None, dropout=None):\n",
    "    # Shape of q and k are the same, both are (batch_size, seq_len, d_k)\n",
    "    # Shape of v is (batch_size, seq_len, d_v)\n",
    "    attention_scores = torch.matmul(q, k.transpose(-2, -1))/math.sqrt(q.shape[-1])  # size (batch_size, seq_len, seq_len)\n",
    "    \n",
    "    # Apply mask to scores\n",
    "    # <pad>\n",
    "    if mask is not None:\n",
    "        attention_scores = attention_scores.masked_fill(mask == 0, value=-1e9)\n",
    "        \n",
    "    # Softmax along the last dimension\n",
    "    attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "    \n",
    "    if dropout is not None:\n",
    "        attention_weights = dropout(attention_weights)\n",
    "        \n",
    "    output = torch.matmul(attention_weights, v)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Head Attention layer\n",
    "\n",
    "![](images/multi_head_attention.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(torch.nn.Module):\n",
    "    def __init__(self, n_heads, d_model, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_heads = n_heads\n",
    "        self.d_model = d_model\n",
    "        self.d_k = self.d_v = d_model//n_heads\n",
    "        \n",
    "        # self attention linear layers\n",
    "        # Linear layers for q, k, v vectors generation in different heads\n",
    "        self.q_linear_layers = []\n",
    "        self.k_linear_layers = []\n",
    "        self.v_linear_layers = []\n",
    "        for i in range(n_heads):\n",
    "            self.q_linear_layers.append(torch.nn.Linear(d_model, self.d_k))\n",
    "            self.k_linear_layers.append(torch.nn.Linear(d_model, self.d_k))\n",
    "            self.v_linear_layers.append(torch.nn.Linear(d_model, self.d_v))\n",
    "        \n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.out = torch.nn.Linear(n_heads*self.d_v, d_model)\n",
    "        \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        multi_head_attention_outputs = []\n",
    "        for q_linear, k_linear, v_linear in zip(self.q_linear_layers,\n",
    "                                                self.k_linear_layers,\n",
    "                                                self.v_linear_layers):\n",
    "            new_q = q_linear(q)  # size: (batch_size, seq_len, d_k)\n",
    "            new_k = k_linear(k)  # size: (batch_size, seq_len, d_k)\n",
    "            new_v = v_linear(v)  # size: (batch_size, seq_len, d_v)\n",
    "            \n",
    "            # Scaled Dot-Product attention\n",
    "            head_v = scaled_dot_product_attention(new_q, new_k, new_v, mask, self.dropout)  # (batch_size, seq_len, d_v)\n",
    "            multi_head_attention_outputs.append(head_v)\n",
    "            \n",
    "        # Concat\n",
    "        #import pdb; pdb.set_trace()\n",
    "        concat = torch.cat(multi_head_attention_outputs, -1)  # (batch_size, seq_len, n_heads*d_v)\n",
    "        \n",
    "        # Linear layer to recover to original shap\n",
    "        output = self.out(concat)  # (batch_size, seq_len, d_model)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed Forward layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(torch.nn.Module):\n",
    "    def __init__(self, d_model, d_ff=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.linear_1 = torch.nn.Linear(d_model, d_ff)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.linear_2 = torch.nn.Linear(d_ff, d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.dropout(F.relu(self.linear_1(x)))\n",
    "        x = self.linear_2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Normalization layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalization\n",
    "\n",
    "$$\\mu = \\frac{1}{m} \\sum_{i=1}^{m}x_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\sigma^{2} = \\frac{1}{m} \\sum^{m}_{i=1}(x_{i} - \\mu)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{Z}_i = \\frac{x_i - \\mu_i}{\\sqrt{\\sigma^{2}_{i} + \\epsilon}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add two learnable parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\tilde{Z}_i = \\alpha_i * \\hat{Z}_i + \\beta_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(torch.nn.Module):\n",
    "    def __init__(self, d_model, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.alpha = torch.nn.Parameter(torch.ones(self.d_model))\n",
    "        self.beta = torch.nn.Parameter(torch.zeros(self.d_model))\n",
    "        self.eps = eps\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x size: (batch_size, seq_len, d_model)\n",
    "        x_hat = (x - x.mean(dim=-1, keepdim=True))/(x.std(dim=-1, keepdim=True) + self.eps)\n",
    "        x_tilde = self.alpha*x_hat + self.beta\n",
    "        return x_tilde"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder & Decoder layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder layer\n",
    "\n",
    "An encoder layer contains a multi-head attention layer and feed forward layer\n",
    "\n",
    "![](images/encoder.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(torch.nn.Module):\n",
    "    def __init__(self, d_model, n_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.norm_1 = LayerNorm(d_model)\n",
    "        self.norm_2 = LayerNorm(d_model)\n",
    "        self.multi_head_attention = MultiHeadAttention(n_heads, d_model)\n",
    "        self.feed_forward = FeedForward(d_model)\n",
    "        self.dropout_1 = torch.nn.Dropout(dropout)\n",
    "        self.dropout_2 = torch.nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        #import pdb; pdb.set_trace()\n",
    "        x = x + self.dropout_1(self.multi_head_attention(x, x, x, mask))\n",
    "        x = self.norm_1(x)\n",
    "        \n",
    "        x = x + self.dropout_2(self.feed_forward(x))\n",
    "        x = self.norm_2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder layer\n",
    "\n",
    "An decoder layer contains two multi-head attention layers and one feed forward layer\n",
    "\n",
    "![](images/decoder.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(torch.nn.Module):\n",
    "    def __init__(self, d_model, n_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm_1 = LayerNorm(d_model)\n",
    "        self.norm_2 = LayerNorm(d_model)\n",
    "        self.norm_3 = LayerNorm(d_model)\n",
    "        \n",
    "        self.dropout_1 = torch.nn.Dropout(dropout)\n",
    "        self.dropout_2 = torch.nn.Dropout(dropout)\n",
    "        self.dropout_3 = torch.nn.Dropout(dropout)\n",
    "        \n",
    "        self.multi_head_attention_1 = MultiHeadAttention(n_heads, d_model)\n",
    "        self.multi_head_attention_2 = MultiHeadAttention(n_heads, d_model)\n",
    "        \n",
    "        self.feed_forward = FeedForward(d_model)\n",
    "        \n",
    "    def forward(self, x, encoder_output, src_mask, trg_mask):\n",
    "        x = self.dropout_1(self.multi_head_attention_1(x, x, x, trg_mask))\n",
    "        x = x + self.norm_1(x)\n",
    "        \n",
    "        x = self.dropout_2(self.multi_head_attention_2(x, encoder_output, encoder_output, src_mask))\n",
    "        x = x + self.norm_2(x)\n",
    "        \n",
    "        x = self.dropout_3(self.feed_forward(x))\n",
    "        x = x + self.norm_3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def clone_layer(module, N):\n",
    "    return torch.nn.ModuleList([copy.deepcopy(module) for i in range(N)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder & Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, N, n_heads):\n",
    "        super().__init__()\n",
    "        self.embed = Embedder(vocab_size, d_model)\n",
    "        self.pe = PositionalEncoder(d_model)\n",
    "        self.encoder_layers = clone_layer(EncoderLayer(d_model, n_heads), N)\n",
    "        self.norm = LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, src, mask):\n",
    "        x = self.embed(src)\n",
    "        x = self.pe(x)\n",
    "        for encoder in self.encoder_layers:\n",
    "            x = encoder(x, mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, N, n_heads):\n",
    "        super().__init__()\n",
    "        self.embed = Embedder(vocab_size, d_model)\n",
    "        self.pe = PositionalEncoder(d_model)\n",
    "        self.decoder_layers = clone_layer(DecoderLayer(d_model, n_heads), N)\n",
    "        self.norm = LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, trg, encoder_output, src_mask, trg_mask):\n",
    "        x = self.embed(trg)\n",
    "        x = self.pe(x)\n",
    "        for decoder in self.decoder_layers:\n",
    "            x = decoder(x, encoder_output, src_mask, trg_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(torch.nn.Module):\n",
    "    def __init__(self, src_vocab_size, trg_vocab_size, d_model, N, n_heads):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(src_vocab_size, d_model, N, n_heads)\n",
    "        self.decoder = Decoder(trg_vocab_size, d_model, N, n_heads)\n",
    "        self.linear = torch.nn.Linear(d_model, trg_vocab_size)\n",
    "        \n",
    "    def forward(self, src, trg, src_mask, trg_mask):\n",
    "        encoder_output = self.encoder(src, src_mask)\n",
    "        decoder_output = self.decoder(trg, encoder_output, src_mask, trg_mask)\n",
    "        output = self.linear(decoder_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from torchtext import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('pt_core_news_sm')\n",
    "\n",
    "tokenizer = lambda sentence: [tok.text for tok in nlp.tokenizer(sentence) if tok.text != \" \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC = data.Field(lower=True, tokenize=tokenizer)\n",
    "TRG = data.Field(lower=True, tokenize=tokenizer, init_token='<sos>', eos_token='<eos>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_data = open('data/english.txt', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "trg_data = open('data/french.txt', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = {'src': [line for line in src_data], 'trg': [line for line in trg_data]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(raw_data, columns=['src', 'trg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>trg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go.\\n</td>\n",
       "      <td>Va !\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Run!\\n</td>\n",
       "      <td>Cours !\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Run!\\n</td>\n",
       "      <td>Courez !\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fire!\\n</td>\n",
       "      <td>Au feu !\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Help!\\n</td>\n",
       "      <td>À l'aide !\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Jump.\\n</td>\n",
       "      <td>Saute.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Stop!\\n</td>\n",
       "      <td>Ça suffit !\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Stop!\\n</td>\n",
       "      <td>Stop !\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Stop!\\n</td>\n",
       "      <td>Arrête-toi !\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Wait!\\n</td>\n",
       "      <td>Attends !\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Wait!\\n</td>\n",
       "      <td>Attendez !\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Go on.\\n</td>\n",
       "      <td>Poursuis.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Go on.\\n</td>\n",
       "      <td>Continuez.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Go on.\\n</td>\n",
       "      <td>Poursuivez.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>I see.\\n</td>\n",
       "      <td>Je comprends.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>I try.\\n</td>\n",
       "      <td>J'essaye.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>I won!\\n</td>\n",
       "      <td>J'ai gagné !\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>I won!\\n</td>\n",
       "      <td>Je l'ai emporté !\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Oh no!\\n</td>\n",
       "      <td>Oh non !\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Attack!\\n</td>\n",
       "      <td>Attaque !\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Attack!\\n</td>\n",
       "      <td>Attaquez !\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Cheers!\\n</td>\n",
       "      <td>Santé !\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Cheers!\\n</td>\n",
       "      <td>À votre santé !\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Cheers!\\n</td>\n",
       "      <td>Merci !\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Cheers!\\n</td>\n",
       "      <td>Tchin-tchin !\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Get up.\\n</td>\n",
       "      <td>Lève-toi.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Go now.\\n</td>\n",
       "      <td>Va, maintenant.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Go now.\\n</td>\n",
       "      <td>Allez-y maintenant.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Go now.\\n</td>\n",
       "      <td>Vas-y maintenant.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Got it!\\n</td>\n",
       "      <td>J'ai pigé !\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154853</th>\n",
       "      <td>That proposal may be a way to kill two birds w...</td>\n",
       "      <td>Cette proposition pourrait faire d'une pierre ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154854</th>\n",
       "      <td>A committee is a group of people who individua...</td>\n",
       "      <td>Un comité est un groupe de gens qui ne peuvent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154855</th>\n",
       "      <td>One of the best ways to help us is to translat...</td>\n",
       "      <td>L'une des meilleures manières de nous aider es...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154856</th>\n",
       "      <td>Since it will be cold soon, it might be nice t...</td>\n",
       "      <td>Puisqu'il fera bientôt froid, ça serait chouet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154857</th>\n",
       "      <td>A man who has never gone to school may steal f...</td>\n",
       "      <td>Un homme qui n'a jamais été à l'école peut vol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154858</th>\n",
       "      <td>One way to lower the number of errors in the T...</td>\n",
       "      <td>Un moyen de diminuer le nombre d’erreurs dans ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154859</th>\n",
       "      <td>What is old age? First you forget names, then ...</td>\n",
       "      <td>Qu'est l'âge ? D'abord on oublie les noms, et ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154860</th>\n",
       "      <td>What is old age? First you forget names, then ...</td>\n",
       "      <td>Ce qu'est l'âge ? D'abord on oublie les noms, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154861</th>\n",
       "      <td>He and I have a near-telepathic understanding ...</td>\n",
       "      <td>Lui et moi avons une compréhension quasi-télép...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154862</th>\n",
       "      <td>Although rainforests make up only two percent ...</td>\n",
       "      <td>Bien que les forêts tropicales ne couvrent que...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154863</th>\n",
       "      <td>If you translate from your second language int...</td>\n",
       "      <td>Si vous traduisez de votre seconde langue dans...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154864</th>\n",
       "      <td>I love trying out new things, so I always buy ...</td>\n",
       "      <td>J'adore essayer de nouvelles choses, alors j'a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154865</th>\n",
       "      <td>A good theory is characterized by the fact tha...</td>\n",
       "      <td>Une bonne théorie se caractérise par le fait d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154866</th>\n",
       "      <td>The more time you spend speaking a foreign lan...</td>\n",
       "      <td>Plus l'on passe de temps à parler une langue é...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154867</th>\n",
       "      <td>The enquiry concluded that, despite his denial...</td>\n",
       "      <td>L'enquête conclut qu'en dépit de ses dénégatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154868</th>\n",
       "      <td>You may not learn to speak as well as a native...</td>\n",
       "      <td>Peut-être n'apprendrez-vous pas à parler comme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154869</th>\n",
       "      <td>And the good news is that today the economy is...</td>\n",
       "      <td>Et la bonne nouvelle est qu'aujourd'hui l'écon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154870</th>\n",
       "      <td>E-cigarettes are being promoted as a healthy a...</td>\n",
       "      <td>La cigarette électronique est mise en avant co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154871</th>\n",
       "      <td>It's still too hard to find a job. And even if...</td>\n",
       "      <td>C'est encore trop difficile de trouver un empl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154872</th>\n",
       "      <td>Even at the end of the nineteenth century, sai...</td>\n",
       "      <td>Même à la fin du dix-neuvième siècle, les mari...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154873</th>\n",
       "      <td>Five tremors in excess of magnitude 5.0 on the...</td>\n",
       "      <td>Cinq secousses dépassant la magnitude cinq sur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154874</th>\n",
       "      <td>No matter how much you try to convince people ...</td>\n",
       "      <td>Peu importe le temps que tu passeras à essayer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154875</th>\n",
       "      <td>We need to uphold laws against discrimination ...</td>\n",
       "      <td>Nous devons faire respecter les lois contre la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154876</th>\n",
       "      <td>A child who is a native speaker usually knows ...</td>\n",
       "      <td>Un enfant qui est un locuteur natif connaît ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154877</th>\n",
       "      <td>There are four main causes of alcohol-related ...</td>\n",
       "      <td>Il y a quatre causes principales de décès liés...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154878</th>\n",
       "      <td>\"Top-down economics never works,\" said Obama. ...</td>\n",
       "      <td>« L'économie en partant du haut vers le bas, ç...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154879</th>\n",
       "      <td>A carbon footprint is the amount of carbon dio...</td>\n",
       "      <td>Une empreinte carbone est la somme de pollutio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154880</th>\n",
       "      <td>Death is something that we're often discourage...</td>\n",
       "      <td>La mort est une chose qu'on nous décourage sou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154881</th>\n",
       "      <td>Since there are usually multiple websites on a...</td>\n",
       "      <td>Puisqu'il y a de multiples sites web sur chaqu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154882</th>\n",
       "      <td>If someone who doesn't know your background sa...</td>\n",
       "      <td>Si quelqu'un qui ne connaît pas vos antécédent...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>154883 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      src  \\\n",
       "0                                                   Go.\\n   \n",
       "1                                                  Run!\\n   \n",
       "2                                                  Run!\\n   \n",
       "3                                                 Fire!\\n   \n",
       "4                                                 Help!\\n   \n",
       "5                                                 Jump.\\n   \n",
       "6                                                 Stop!\\n   \n",
       "7                                                 Stop!\\n   \n",
       "8                                                 Stop!\\n   \n",
       "9                                                 Wait!\\n   \n",
       "10                                                Wait!\\n   \n",
       "11                                               Go on.\\n   \n",
       "12                                               Go on.\\n   \n",
       "13                                               Go on.\\n   \n",
       "14                                               I see.\\n   \n",
       "15                                               I try.\\n   \n",
       "16                                               I won!\\n   \n",
       "17                                               I won!\\n   \n",
       "18                                               Oh no!\\n   \n",
       "19                                              Attack!\\n   \n",
       "20                                              Attack!\\n   \n",
       "21                                              Cheers!\\n   \n",
       "22                                              Cheers!\\n   \n",
       "23                                              Cheers!\\n   \n",
       "24                                              Cheers!\\n   \n",
       "25                                              Get up.\\n   \n",
       "26                                              Go now.\\n   \n",
       "27                                              Go now.\\n   \n",
       "28                                              Go now.\\n   \n",
       "29                                              Got it!\\n   \n",
       "...                                                   ...   \n",
       "154853  That proposal may be a way to kill two birds w...   \n",
       "154854  A committee is a group of people who individua...   \n",
       "154855  One of the best ways to help us is to translat...   \n",
       "154856  Since it will be cold soon, it might be nice t...   \n",
       "154857  A man who has never gone to school may steal f...   \n",
       "154858  One way to lower the number of errors in the T...   \n",
       "154859  What is old age? First you forget names, then ...   \n",
       "154860  What is old age? First you forget names, then ...   \n",
       "154861  He and I have a near-telepathic understanding ...   \n",
       "154862  Although rainforests make up only two percent ...   \n",
       "154863  If you translate from your second language int...   \n",
       "154864  I love trying out new things, so I always buy ...   \n",
       "154865  A good theory is characterized by the fact tha...   \n",
       "154866  The more time you spend speaking a foreign lan...   \n",
       "154867  The enquiry concluded that, despite his denial...   \n",
       "154868  You may not learn to speak as well as a native...   \n",
       "154869  And the good news is that today the economy is...   \n",
       "154870  E-cigarettes are being promoted as a healthy a...   \n",
       "154871  It's still too hard to find a job. And even if...   \n",
       "154872  Even at the end of the nineteenth century, sai...   \n",
       "154873  Five tremors in excess of magnitude 5.0 on the...   \n",
       "154874  No matter how much you try to convince people ...   \n",
       "154875  We need to uphold laws against discrimination ...   \n",
       "154876  A child who is a native speaker usually knows ...   \n",
       "154877  There are four main causes of alcohol-related ...   \n",
       "154878  \"Top-down economics never works,\" said Obama. ...   \n",
       "154879  A carbon footprint is the amount of carbon dio...   \n",
       "154880  Death is something that we're often discourage...   \n",
       "154881  Since there are usually multiple websites on a...   \n",
       "154882  If someone who doesn't know your background sa...   \n",
       "\n",
       "                                                      trg  \n",
       "0                                                  Va !\\n  \n",
       "1                                               Cours !\\n  \n",
       "2                                              Courez !\\n  \n",
       "3                                              Au feu !\\n  \n",
       "4                                            À l'aide !\\n  \n",
       "5                                                Saute.\\n  \n",
       "6                                           Ça suffit !\\n  \n",
       "7                                                Stop !\\n  \n",
       "8                                          Arrête-toi !\\n  \n",
       "9                                             Attends !\\n  \n",
       "10                                           Attendez !\\n  \n",
       "11                                            Poursuis.\\n  \n",
       "12                                           Continuez.\\n  \n",
       "13                                          Poursuivez.\\n  \n",
       "14                                        Je comprends.\\n  \n",
       "15                                            J'essaye.\\n  \n",
       "16                                         J'ai gagné !\\n  \n",
       "17                                    Je l'ai emporté !\\n  \n",
       "18                                             Oh non !\\n  \n",
       "19                                            Attaque !\\n  \n",
       "20                                           Attaquez !\\n  \n",
       "21                                              Santé !\\n  \n",
       "22                                      À votre santé !\\n  \n",
       "23                                              Merci !\\n  \n",
       "24                                        Tchin-tchin !\\n  \n",
       "25                                            Lève-toi.\\n  \n",
       "26                                      Va, maintenant.\\n  \n",
       "27                                  Allez-y maintenant.\\n  \n",
       "28                                    Vas-y maintenant.\\n  \n",
       "29                                          J'ai pigé !\\n  \n",
       "...                                                   ...  \n",
       "154853  Cette proposition pourrait faire d'une pierre ...  \n",
       "154854  Un comité est un groupe de gens qui ne peuvent...  \n",
       "154855  L'une des meilleures manières de nous aider es...  \n",
       "154856  Puisqu'il fera bientôt froid, ça serait chouet...  \n",
       "154857  Un homme qui n'a jamais été à l'école peut vol...  \n",
       "154858  Un moyen de diminuer le nombre d’erreurs dans ...  \n",
       "154859  Qu'est l'âge ? D'abord on oublie les noms, et ...  \n",
       "154860  Ce qu'est l'âge ? D'abord on oublie les noms, ...  \n",
       "154861  Lui et moi avons une compréhension quasi-télép...  \n",
       "154862  Bien que les forêts tropicales ne couvrent que...  \n",
       "154863  Si vous traduisez de votre seconde langue dans...  \n",
       "154864  J'adore essayer de nouvelles choses, alors j'a...  \n",
       "154865  Une bonne théorie se caractérise par le fait d...  \n",
       "154866  Plus l'on passe de temps à parler une langue é...  \n",
       "154867  L'enquête conclut qu'en dépit de ses dénégatio...  \n",
       "154868  Peut-être n'apprendrez-vous pas à parler comme...  \n",
       "154869  Et la bonne nouvelle est qu'aujourd'hui l'écon...  \n",
       "154870  La cigarette électronique est mise en avant co...  \n",
       "154871  C'est encore trop difficile de trouver un empl...  \n",
       "154872  Même à la fin du dix-neuvième siècle, les mari...  \n",
       "154873  Cinq secousses dépassant la magnitude cinq sur...  \n",
       "154874  Peu importe le temps que tu passeras à essayer...  \n",
       "154875  Nous devons faire respecter les lois contre la...  \n",
       "154876  Un enfant qui est un locuteur natif connaît ha...  \n",
       "154877  Il y a quatre causes principales de décès liés...  \n",
       "154878  « L'économie en partant du haut vers le bas, ç...  \n",
       "154879  Une empreinte carbone est la somme de pollutio...  \n",
       "154880  La mort est une chose qu'on nous décourage sou...  \n",
       "154881  Puisqu'il y a de multiples sites web sur chaqu...  \n",
       "154882  Si quelqu'un qui ne connaît pas vos antécédent...  \n",
       "\n",
       "[154883 rows x 2 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('en_to_fr.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md                en_to_fr.csv             test.ipynb\r\n",
      "\u001b[1m\u001b[36mdata\u001b[m\u001b[m/                    \u001b[1m\u001b[36mimages\u001b[m\u001b[m/                  transformer_model.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fields = [('src', SRC), ('trg', TRG)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = data.TabularDataset('./en_to_fr.csv', format='csv', fields=data_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC.build_vocab(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14114"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(SRC.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRG.build_vocab(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28353"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(TRG.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torchtext.data.dataset.TabularDataset at 0x123bcbe48>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set some parameters\n",
    "d_model = 512\n",
    "n_heads = 8\n",
    "N = 6\n",
    "src_vocab_size = len(SRC.vocab)\n",
    "trg_vocab_size = len(TRG.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(src_vocab_size, trg_vocab_size, d_model, N, n_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zjshao/.pyenv/versions/3.5.3/envs/ml/lib/python3.5/site-packages/ipykernel_launcher.py:3: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "for p in model.parameters():\n",
    "    if p.dim() > 1:\n",
    "        torch.nn.init.xavier_uniform(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = data.Iterator(train_set, batch_size=32, sort_key=lambda x: (len(x.src), len(x.trg)), shuffle=True, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def create_mask(src_input, trg_input):\n",
    "    # Source input mask\n",
    "    pad = SRC.vocab.stoi['<pad>']\n",
    "    src_mask = (src_input != pad).unsqueeze(1)\n",
    "    \n",
    "    # Target input mask\n",
    "    trg_mask = (trg_input != pad).unsqueeze(1)\n",
    "    \n",
    "    seq_len = trg_input.size(1)\n",
    "    nopeak_mask = np.tril(np.ones((1, seq_len, seq_len)), k=0).astype('uint8')\n",
    "    nopeak_mask = torch.from_numpy(nopeak_mask) != 0\n",
    "    trg_mask = trg_mask & nopeak_mask\n",
    "    \n",
    "    return src_mask, trg_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train_model(n_epochs, output_interval=100):\n",
    "    model.train()\n",
    "    start = time.time()\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        total_loss = 0\n",
    "        for i, batch in enumerate(train_iter):\n",
    "            \n",
    "            src_input = batch.src.transpose(0, 1)  # size (batch_size, seq_len)\n",
    "            trg = batch.trg.transpose(0, 1)  # size (batch_size, seq_len)\n",
    "            \n",
    "            trg_input = trg[:, :-1]\n",
    "            ys = trg[:, 1:].contiguous().view(-1)\n",
    "            \n",
    "            # create src & trg masks\n",
    "            src_mask, trg_mask = create_mask(src_input, trg_input)\n",
    "            preds = model(src_input, trg_input, src_mask, trg_mask)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss = F.cross_entropy(preds.view(-1, preds.size(-1)), ys, ignore_index=1)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.data[0]\n",
    "\n",
    "            if (i + 1) % output_interval == 0:\n",
    "                avg_loss = total_loss/output_interval\n",
    "                print('time = {}, epoch = {}, iter = {}, loss = {}'.format((time.time() - start)/60,\n",
    "                                                                           epoch + 1,\n",
    "                                                                           i + 1,\n",
    "                                                                           avg_loss))\n",
    "                total_loss = 0\n",
    "                start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zjshao/.pyenv/versions/3.5.3/envs/ml/lib/python3.5/site-packages/ipykernel_launcher.py:27: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time = 0.07368873357772827, epoch = 1, iter = 1, loss = 10.275301933288574\n",
      "time = 0.05243396759033203, epoch = 1, iter = 2, loss = 10.036077499389648\n",
      "time = 0.05758536656697591, epoch = 1, iter = 3, loss = 9.876063346862793\n",
      "time = 0.059804431597391766, epoch = 1, iter = 4, loss = 9.840919494628906\n",
      "time = 0.06737850109736125, epoch = 1, iter = 5, loss = 9.733710289001465\n",
      "time = 0.060716116428375246, epoch = 1, iter = 6, loss = 9.621753692626953\n",
      "time = 0.06828871568044027, epoch = 1, iter = 7, loss = 9.605151176452637\n",
      "time = 0.06440561612447103, epoch = 1, iter = 8, loss = 9.512434959411621\n",
      "time = 0.06431623299916585, epoch = 1, iter = 9, loss = 9.478424072265625\n",
      "time = 0.06724570194880168, epoch = 1, iter = 10, loss = 9.49864387512207\n",
      "time = 0.058811533451080325, epoch = 1, iter = 11, loss = 9.33305835723877\n",
      "time = 0.061342501640319826, epoch = 1, iter = 12, loss = 9.282251358032227\n",
      "time = 0.07716486851374309, epoch = 1, iter = 13, loss = 9.268787384033203\n",
      "time = 0.06068573395411173, epoch = 1, iter = 14, loss = 9.16775131225586\n",
      "time = 0.0706778327624003, epoch = 1, iter = 15, loss = 9.151176452636719\n",
      "time = 0.07176573276519775, epoch = 1, iter = 16, loss = 9.124017715454102\n",
      "time = 0.07577134768168131, epoch = 1, iter = 17, loss = 9.080439567565918\n",
      "time = 0.07278071641921997, epoch = 1, iter = 18, loss = 9.008810997009277\n",
      "time = 0.05596356789271037, epoch = 1, iter = 19, loss = 8.948866844177246\n",
      "time = 0.06370005210240683, epoch = 1, iter = 20, loss = 8.894721984863281\n",
      "time = 0.0560766339302063, epoch = 1, iter = 21, loss = 8.81960678100586\n",
      "time = 0.11069205204645792, epoch = 1, iter = 22, loss = 8.837837219238281\n",
      "time = 0.07233175039291381, epoch = 1, iter = 23, loss = 8.66609001159668\n",
      "time = 0.05656316677729289, epoch = 1, iter = 24, loss = 8.57186222076416\n",
      "time = 0.0599276860555013, epoch = 1, iter = 25, loss = 8.567977905273438\n",
      "time = 0.09096531470616659, epoch = 1, iter = 26, loss = 8.546257019042969\n",
      "time = 0.07039418220520019, epoch = 1, iter = 27, loss = 8.51272201538086\n",
      "time = 0.08430409828821818, epoch = 1, iter = 28, loss = 8.435883522033691\n",
      "time = 0.09260969956715902, epoch = 1, iter = 29, loss = 8.334375381469727\n",
      "time = 0.06071858406066895, epoch = 1, iter = 30, loss = 8.334733009338379\n",
      "time = 0.064847465356191, epoch = 1, iter = 31, loss = 8.259394645690918\n",
      "time = 0.10941791534423828, epoch = 1, iter = 32, loss = 8.203652381896973\n",
      "time = 0.07142060200373332, epoch = 1, iter = 33, loss = 8.162467002868652\n",
      "time = 0.06419503291447957, epoch = 1, iter = 34, loss = 8.033669471740723\n",
      "time = 0.09461748202641805, epoch = 1, iter = 35, loss = 8.166277885437012\n",
      "time = 0.07295833031336467, epoch = 1, iter = 36, loss = 8.03286361694336\n",
      "time = 0.08415588537851969, epoch = 1, iter = 37, loss = 7.826048374176025\n",
      "time = 0.08624071677525838, epoch = 1, iter = 38, loss = 7.89840841293335\n",
      "time = 0.07309126456578573, epoch = 1, iter = 39, loss = 7.866915702819824\n",
      "time = 0.06748514970143636, epoch = 1, iter = 40, loss = 7.68775749206543\n",
      "time = 0.0692827820777893, epoch = 1, iter = 41, loss = 7.623300552368164\n",
      "time = 0.06189734935760498, epoch = 1, iter = 42, loss = 7.6679487228393555\n",
      "time = 0.08047765096028646, epoch = 1, iter = 43, loss = 7.521366119384766\n",
      "time = 0.0684423009554545, epoch = 1, iter = 44, loss = 7.446471691131592\n",
      "time = 0.07436726490656535, epoch = 1, iter = 45, loss = 7.616914749145508\n",
      "time = 0.06415958007176717, epoch = 1, iter = 46, loss = 7.335622310638428\n",
      "time = 0.057961066563924156, epoch = 1, iter = 47, loss = 7.369991302490234\n",
      "time = 0.06256914933522542, epoch = 1, iter = 48, loss = 7.360755443572998\n",
      "time = 0.07985510031382242, epoch = 1, iter = 49, loss = 7.160858154296875\n",
      "time = 0.08559258381525675, epoch = 1, iter = 50, loss = 7.240228652954102\n",
      "time = 0.08142703374226888, epoch = 1, iter = 51, loss = 7.339879989624023\n",
      "time = 0.0808611512184143, epoch = 1, iter = 52, loss = 7.184959411621094\n",
      "time = 0.06521214644114176, epoch = 1, iter = 53, loss = 7.111505031585693\n",
      "time = 0.07443951765696208, epoch = 1, iter = 54, loss = 7.1437859535217285\n",
      "time = 0.061329265435536705, epoch = 1, iter = 55, loss = 6.9383697509765625\n",
      "time = 0.06178390185038249, epoch = 1, iter = 56, loss = 7.023250102996826\n",
      "time = 0.07523408333460489, epoch = 1, iter = 57, loss = 6.823822021484375\n",
      "time = 0.07574890057245891, epoch = 1, iter = 58, loss = 7.063283443450928\n",
      "time = 0.06116951704025268, epoch = 1, iter = 59, loss = 6.806905746459961\n",
      "time = 0.04993902047475179, epoch = 1, iter = 60, loss = 6.84621524810791\n",
      "time = 0.06731653610865275, epoch = 1, iter = 61, loss = 6.785461902618408\n",
      "time = 0.05475561618804932, epoch = 1, iter = 62, loss = 6.7967143058776855\n",
      "time = 0.06382323106129964, epoch = 1, iter = 63, loss = 6.607127666473389\n",
      "time = 0.061555349826812746, epoch = 1, iter = 64, loss = 6.847414493560791\n",
      "time = 0.07733279863993327, epoch = 1, iter = 65, loss = 6.746364593505859\n",
      "time = 0.0634850025177002, epoch = 1, iter = 66, loss = 6.606636047363281\n",
      "time = 0.07392118374506633, epoch = 1, iter = 67, loss = 6.816209316253662\n",
      "time = 0.06916069984436035, epoch = 1, iter = 68, loss = 6.543198585510254\n",
      "time = 0.06224756638209025, epoch = 1, iter = 69, loss = 6.31641149520874\n",
      "time = 0.06160989999771118, epoch = 1, iter = 70, loss = 6.547100067138672\n",
      "time = 0.06779568592707316, epoch = 1, iter = 71, loss = 6.4370036125183105\n",
      "time = 0.06392256418863933, epoch = 1, iter = 72, loss = 6.589992046356201\n",
      "time = 0.08541620175043742, epoch = 1, iter = 73, loss = 6.499094009399414\n",
      "time = 0.09481403430302938, epoch = 1, iter = 74, loss = 6.3581223487854\n",
      "time = 0.059490633010864255, epoch = 1, iter = 75, loss = 6.293521404266357\n",
      "time = 0.07245870033899943, epoch = 1, iter = 76, loss = 6.472769260406494\n",
      "time = 0.05217263698577881, epoch = 1, iter = 77, loss = 6.150349140167236\n",
      "time = 0.10459943215052286, epoch = 1, iter = 78, loss = 6.43544340133667\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-09835295c489>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-38-f9d81b73299c>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(n_epochs, output_interval)\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.5.3/envs/ml/lib/python3.5/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.5.3/envs/ml/lib/python3.5/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_model(3, output_interval=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
